step_normalize(all_numeric_predictors()) %>%
step_pca(all_numeric_predictors(), keep_original_cols = FALSE, num_comp = 10)
pipe_preprocessing3 <-
recipe(last_sale_price ~ ., data = data_train) %>%
update_role(global_index, new_role = "ID") %>%
step_impute_median(all_numeric_predictors()) %>%
step_impute_mode(all_nominal_predictors()) %>%
step_date(c(last_sale_date, creation_date), keep_original_cols = FALSE,
features = c("dow", "doy", "week", "month", "year", "quarter", "semester")) %>%
step_orderNorm(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_numeric_predictors(), keep_original_cols = FALSE, num_comp = 10)
set.seed(2023)
my_grid <- grid_max_entropy(penalty(), mixture(), size = 25)
set.seed(2023)
my_grid <- grid_max_entropy(penalty(), mixture(), size = 5)
my_control <- control_grid(parallel_over = "everything",
save_pred = TRUE)
registerDoParallel()
my_wflow_set <-
workflow_set(
preproc = list(
pipe1 = pipe_preprocessing1,
pipe2 = pipe_preprocessing2,
pipe3 = pipe_preprocessing3
),
models = list(regularized_reg = my_model)
) %>%
workflow_map(
verbose = TRUE,
seed = 2023,
resamples = folds,
grid = my_grid,
metrics = metric_set(rmse),
control = my_control
)
stopImplicitCluster()
my_wflow_set %>%
rank_results()
knitr::opts_chunk$set(fig.align = "center", warning = FALSE, message = FALSE)
library(tidyverse)
library(tsibble)
library(tidymodels)
library(bestNormalize)
library(doParallel)
library(workflowsets)
library(vip)
library(DALEX)
library(DALEXtra)
# Configuración de tema ggplot2 y colores
theme_set(theme_minimal())
source("functions/01-importData.R")
# Parameters for the function
file_collections <- "../data/raw/collections.csv"
file_twitter <- "../data/raw/collections_twitter_stats.csv"
file_train <- "../data/raw/nfts_train.csv"
file_predict <- "../data/raw/nfts_predict.csv"
file_export <- FALSE
# Run function importData()
data_full <- importData(
path_collections = file_collections,
path_twitter = file_twitter,
path_train = file_train,
path_predict = file_predict,
export = file_export
)
df_train <- data_full$train %>%
select(-c(nft_id, collection_id, has_website, has_own_twitter, platform_fees)) %>%
relocate(global_index, last_sale_price, everything()) %>%
mutate(month_diff = as.numeric((last_sale_date - creation_date) / 30))
df_train %>% head()
df_test <- data_full$test %>%
select(-c(nft_id, collection_id, has_website, has_own_twitter, platform_fees)) %>%
relocate(global_index, everything()) %>%
mutate(month_diff = as.numeric((last_sale_date - creation_date) / 30))
df_test %>% head()
# Initial partition
set.seed(2023)
my_split <-
initial_split(data = df_train,
prop = 0.80,
strata = last_sale_price)
data_train <- training(my_split) %>%
slice(1:100)
data_test <- testing(my_split)
# Cross validation with k = 5
set.seed(2023)
folds <- vfold_cv(
data = data_train,
prop = 0.80,
strata = last_sale_price,
v = 5
)
my_model <-
linear_reg(penalty = tune(),
mixture = tune(),
mode = "regression") %>%
set_engine("glmnet")
pipe_preprocessing1 <-
recipe(last_sale_price ~ ., data = data_train) %>%
update_role(global_index, new_role = "ID") %>%
step_impute_median(all_numeric_predictors()) %>%
step_impute_mode(all_nominal_predictors()) %>%
step_date(c(last_sale_date, creation_date), keep_original_cols = FALSE,
features = c("dow", "doy", "week", "month", "year", "quarter", "semester")) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
step_normalize(all_numeric_predictors())
pipe_preprocessing2 <-
recipe(last_sale_price ~ ., data = data_train) %>%
update_role(global_index, new_role = "ID") %>%
step_impute_median(all_numeric_predictors()) %>%
step_impute_mode(all_nominal_predictors()) %>%
step_date(c(last_sale_date, creation_date), keep_original_cols = FALSE,
features = c("dow", "doy", "week", "month", "year", "quarter", "semester")) %>%
step_YeoJohnson(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_numeric_predictors(), keep_original_cols = FALSE, num_comp = 10)
pipe_preprocessing3 <-
recipe(last_sale_price ~ ., data = data_train) %>%
update_role(global_index, new_role = "ID") %>%
step_impute_median(all_numeric_predictors()) %>%
step_impute_mode(all_nominal_predictors()) %>%
step_date(c(last_sale_date, creation_date), keep_original_cols = FALSE,
features = c("dow", "doy", "week", "month", "year", "quarter", "semester")) %>%
step_orderNorm(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_numeric_predictors(), keep_original_cols = FALSE, num_comp = 10)
set.seed(2023)
my_grid <- grid_max_entropy(penalty(), mixture(), size = 5)
my_control <- control_grid(parallel_over = "everything",
save_pred = TRUE)
registerDoParallel()
my_wflow_set <-
workflow_set(
preproc = list(
pipe1 = pipe_preprocessing1,
pipe2 = pipe_preprocessing2,
pipe3 = pipe_preprocessing3
),
models = list(regularized_reg = my_model)
) %>%
workflow_map(
verbose = TRUE,
seed = 2023,
resamples = folds,
grid = my_grid,
metrics = metric_set(rmse),
control = my_control
)
stopImplicitCluster()
my_wflow_set %>%
rank_results()
my_control <- control_grid(parallel_over = "everything",
save_pred = TRUE)
registerDoParallel()
my_wflow_set <-
workflow_set(
preproc = list(
pipe1 = pipe_preprocessing1,
pipe2 = pipe_preprocessing2
),
models = list(regularized_reg = my_model)
) %>%
workflow_map(
verbose = TRUE,
seed = 2023,
resamples = folds,
grid = my_grid,
metrics = metric_set(rmse),
control = my_control
)
stopImplicitCluster()
my_wflow_set %>%
rank_results()
knitr::opts_chunk$set(fig.align = "center", warning = FALSE, message = FALSE)
library(tidyverse)
library(tsibble)
library(tidymodels)
library(bestNormalize)
library(doParallel)
library(workflowsets)
library(vip)
library(DALEX)
library(DALEXtra)
# Configuración de tema ggplot2 y colores
theme_set(theme_minimal())
source("functions/01-importData.R")
# Parameters for the function
file_collections <- "../data/raw/collections.csv"
file_twitter <- "../data/raw/collections_twitter_stats.csv"
file_train <- "../data/raw/nfts_train.csv"
file_predict <- "../data/raw/nfts_predict.csv"
file_export <- FALSE
# Run function importData()
data_full <- importData(
path_collections = file_collections,
path_twitter = file_twitter,
path_train = file_train,
path_predict = file_predict,
export = file_export
)
df_train <- data_full$train %>%
select(-c(nft_id, collection_id, has_website, has_own_twitter, platform_fees)) %>%
relocate(global_index, last_sale_price, everything()) %>%
mutate(month_diff = as.numeric((last_sale_date - creation_date) / 30),
last_sale_price = log10(last_sale_price))
df_train %>% head()
df_test <- data_full$test %>%
select(-c(nft_id, collection_id, has_website, has_own_twitter, platform_fees)) %>%
relocate(global_index, everything()) %>%
mutate(month_diff = as.numeric((last_sale_date - creation_date) / 30))
df_test %>% head()
# Initial partition
set.seed(2023)
my_split <-
initial_split(data = df_train,
prop = 0.80,
strata = last_sale_price)
data_train <- training(my_split) %>%
slice(1:100)
data_test <- testing(my_split)
# Cross validation with k = 5
set.seed(2023)
folds <- vfold_cv(
data = data_train,
prop = 0.80,
strata = last_sale_price,
v = 5
)
my_model <-
linear_reg(penalty = tune(),
mixture = tune(),
mode = "regression") %>%
set_engine("glmnet")
pipe_preprocessing1 <-
recipe(last_sale_price ~ ., data = data_train) %>%
update_role(global_index, new_role = "ID") %>%
step_impute_median(all_numeric_predictors()) %>%
step_impute_mode(all_nominal_predictors()) %>%
step_date(c(last_sale_date, creation_date), keep_original_cols = FALSE,
features = c("dow", "doy", "week", "month", "year", "quarter", "semester")) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
step_normalize(all_numeric_predictors())
pipe_preprocessing2 <-
recipe(last_sale_price ~ ., data = data_train) %>%
update_role(global_index, new_role = "ID") %>%
step_impute_median(all_numeric_predictors()) %>%
step_impute_mode(all_nominal_predictors()) %>%
step_date(c(last_sale_date, creation_date), keep_original_cols = FALSE,
features = c("dow", "doy", "week", "month", "year", "quarter", "semester")) %>%
step_YeoJohnson(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_numeric_predictors(), keep_original_cols = FALSE, num_comp = 10)
pipe_preprocessing3 <-
recipe(last_sale_price ~ ., data = data_train) %>%
update_role(global_index, new_role = "ID") %>%
step_impute_median(all_numeric_predictors()) %>%
step_impute_mode(all_nominal_predictors()) %>%
step_date(c(last_sale_date, creation_date), keep_original_cols = FALSE,
features = c("dow", "doy", "week", "month", "year", "quarter", "semester")) %>%
step_orderNorm(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_numeric_predictors(), keep_original_cols = FALSE, num_comp = 10)
set.seed(2023)
my_grid <- grid_max_entropy(penalty(), mixture(), size = 5)
my_control <- control_grid(parallel_over = "everything",
save_pred = TRUE)
registerDoParallel()
my_wflow_set <-
workflow_set(
preproc = list(
pipe1 = pipe_preprocessing1,
pipe2 = pipe_preprocessing2
),
models = list(regularized_reg = my_model)
) %>%
workflow_map(
verbose = TRUE,
seed = 2023,
resamples = folds,
grid = my_grid,
metrics = metric_set(rmse),
control = my_control
)
stopImplicitCluster()
my_wflow_set %>%
rank_results()
my_control <- control_grid(parallel_over = "everything",
save_pred = TRUE)
registerDoParallel()
my_wflow_set <-
workflow_set(
preproc = list(
pipe1 = pipe_preprocessing1,
pipe2 = pipe_preprocessing2,
pipe3 = pipe_preprocessing3
),
models = list(regularized_reg = my_model)
) %>%
workflow_map(
verbose = TRUE,
seed = 2023,
resamples = folds,
grid = my_grid,
metrics = metric_set(rmse),
control = my_control
)
stopImplicitCluster()
my_wflow_set %>%
rank_results()
my_wflow_set$result
View(data_train)
my_wflow_set$result
show_notes(.Last.tune.result)
my_grid
my_grid <- grid_max_entropy(penalty(), mixture(), size = 25)
set.seed(2023)
my_grid <- grid_max_entropy(penalty(), mixture(), size = 25)
my_grid
knitr::opts_chunk$set(fig.align = "center", warning = FALSE, message = FALSE)
library(tidyverse)
library(tsibble)
library(tidymodels)
library(bestNormalize)
library(doParallel)
library(workflowsets)
library(vip)
library(DALEX)
library(DALEXtra)
# Configuración de tema ggplot2 y colores
theme_set(theme_minimal())
source("functions/01-importData.R")
# Parameters for the function
file_collections <- "../data/raw/collections.csv"
file_twitter <- "../data/raw/collections_twitter_stats.csv"
file_train <- "../data/raw/nfts_train.csv"
file_predict <- "../data/raw/nfts_predict.csv"
file_export <- FALSE
# Run function importData()
data_full <- importData(
path_collections = file_collections,
path_twitter = file_twitter,
path_train = file_train,
path_predict = file_predict,
export = file_export
)
df_train <- data_full$train %>%
select(-c(nft_id, collection_id, has_website, has_own_twitter, platform_fees)) %>%
relocate(global_index, last_sale_price, everything()) %>%
mutate(month_diff = as.numeric((last_sale_date - creation_date) / 30),
last_sale_price = log10(last_sale_price))
df_train %>% head()
df_test <- data_full$test %>%
select(-c(nft_id, collection_id, has_website, has_own_twitter, platform_fees)) %>%
relocate(global_index, everything()) %>%
mutate(month_diff = as.numeric((last_sale_date - creation_date) / 30))
df_test %>% head()
# Initial partition
set.seed(2023)
my_split <-
initial_split(data = df_train,
prop = 0.80,
strata = last_sale_price)
data_train <- training(my_split)
data_test <- testing(my_split)
# Cross validation with k = 5
set.seed(2023)
folds <- vfold_cv(
data = data_train,
prop = 0.80,
strata = last_sale_price,
v = 5
)
my_model <-
linear_reg(penalty = tune(),
mixture = tune(),
mode = "regression") %>%
set_engine("glmnet")
pipe_preprocessing1 <-
recipe(last_sale_price ~ ., data = data_train) %>%
update_role(global_index, new_role = "ID") %>%
step_impute_median(all_numeric_predictors()) %>%
step_impute_mode(all_nominal_predictors()) %>%
step_date(c(last_sale_date, creation_date), keep_original_cols = FALSE,
features = c("dow", "doy", "week", "month", "year", "quarter", "semester")) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
step_normalize(all_numeric_predictors())
pipe_preprocessing2 <-
recipe(last_sale_price ~ ., data = data_train) %>%
update_role(global_index, new_role = "ID") %>%
step_impute_median(all_numeric_predictors()) %>%
step_impute_mode(all_nominal_predictors()) %>%
step_date(c(last_sale_date, creation_date), keep_original_cols = FALSE,
features = c("dow", "doy", "week", "month", "year", "quarter", "semester")) %>%
step_YeoJohnson(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_numeric_predictors(), keep_original_cols = FALSE, num_comp = 10)
pipe_preprocessing3 <-
recipe(last_sale_price ~ ., data = data_train) %>%
update_role(global_index, new_role = "ID") %>%
step_impute_median(all_numeric_predictors()) %>%
step_impute_mode(all_nominal_predictors()) %>%
step_date(c(last_sale_date, creation_date), keep_original_cols = FALSE,
features = c("dow", "doy", "week", "month", "year", "quarter", "semester")) %>%
step_orderNorm(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_numeric_predictors(), keep_original_cols = FALSE, num_comp = 10)
set.seed(2023)
my_grid <- grid_max_entropy(penalty(), mixture(), size = 25)
result_tuning <- read_rds("results_tuning/01-regularizedRegression.rds")
result_tuning %>%
rank_results()
result_tuning %>%
autoplot(rank_metric = "rmse",
metric = "rmse",
select_best = TRUE) +
geom_text(aes(y = mean + 0.2, label = wflow_id),
angle = 90,
hjust = 1)
result_tuning %>%
autoplot(rank_metric = "rmse",
metric = "rmse",
select_best = TRUE) +
geom_text(aes(y = mean + 0.05, label = wflow_id),
angle = 90,
hjust = 1)
result_tuning %>%
autoplot(rank_metric = "rmse",
metric = "rmse",
select_best = TRUE) +
geom_text(aes(y = mean + 0.1, label = wflow_id),
angle = 90,
hjust = 1)
result_tuning %>%
autoplot(rank_metric = "rmse",
metric = "rmse",
select_best = TRUE) +
geom_text(aes(y = mean + 0.12, label = wflow_id),
angle = 90,
hjust = 1)
result_tuning %>%
autoplot(rank_metric = "rmse",
metric = "rmse",
select_best = TRUE) +
geom_text(aes(y = mean + 0.12, label = wflow_id),
angle = 90,
hjust = 1) +
theme(legend.position = "none")
result_tuning %>%
extract_workflow_set_result("pipe1_regularized_reg") %>%
select_best()
final_pipeline <-
result_tuning %>%
extract_workflow("pipe1_regularized_reg") %>%
finalize_workflow(
result_tuning %>%
extract_workflow_set_result("pipe1_regularized_reg") %>%
select_best(metric = "rmse")
) %>%
last_fit(split = my_split, metrics = metric_set(rmse))
final_pipeline %>%
collect_metrics()
final_pipeline %>%
collect_predictions()
final_pipeline %>%
collect_predictions() %>%
ggplot(aes(x = .pred, y = last_sale_price)) +
geom_point()
final_pipeline %>%
collect_predictions() %>%
ggplot(aes(x = .pred, y = last_sale_price)) +
geom_point(alpha = 0.2, color = "gray20") +
geom_smooth(method = "lm", se = FALSE, color = "firebrick2")
final_pipeline %>%
collect_predictions() %>%
ggplot(aes(x = .pred, y = last_sale_price)) +
geom_point(alpha = 0.05, color = "gray20") +
geom_smooth(method = "lm", se = FALSE, color = "firebrick2")
final_model <- final_pipeline %>%
extract_workflow()
final_model
predict(final_model, new_data = df_test)
log10(2.3)
10^(log10(2.3))
predict_test <- 10^(predict(final_model, new_data = df_test)$.pred)
predict_test %>% range()
8.770817e+02
range(df_train$last_sale_price)
10^-7.188066
10^3.010301
write_rds(x = final_model, file = "final_models/01-regularizedRegressionFinal.rds",
compress = "xz")
df_test %>%
select(global_index) %>%
mutate(last_sale_price = predict_test)
predict_test <- 10^(predict(final_model, new_data = df_test)$.pred)
df_submission <-
df_test %>%
select(global_index) %>%
mutate(last_sale_price = predict_test)
df_submission %>% head()
write_csv(x = df_submission, file = "../data/submission/01-submission.csv")
prueba <- read_rds("final_models/01-regularizedRegressionFinal.rds")
10^(predict(prueba, new_data = df_test)$.pred) %>% head()
setwd("D:/Otros/Github/web-edimer/semillero-ciencia-datos")
