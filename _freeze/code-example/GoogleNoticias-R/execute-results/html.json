{
  "hash": "88e9982a043b66e23a86a3c4cdc5cb70",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Web Scraping con R\"\nsubtitle: \"Google Noticias\"\nauthor: \"Semillero Ciencia de Datos con R y Python\"\nlang: es \nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: auto\nformat:\n  html:\n    page-layout: article\n    fig-width: 6\n    fig-height: 4.5\n    toc: true\n    toc-title: \"Tabla de contenido\"\n    smooth-scroll: true\n    code-fold: true\n    df-print: paged\n    toc-location: left\n    number-depth: 4\n    theme: yeti\n    code-copy: true\n    highlight-style: github\n    code-tools:\n      source: true\n---\n\n\n\n\n# Objetivo\n\nEste documento tiene como propósito ejemplificar técnicas de web scraping con R y análisis exploratorio de datos.\n\n# Requisitos previos\n\nPara ejecutar este documento es necesario tener instalado lo siguiente:\n\n- [`R`](https://cran.r-project.org/)\n- [`RStudio`](https://posit.co/downloads/)\n- [`Quarto`](https://quarto.org/)\n\nPara garantizar la reproducibilidad de este documento es necesario instalar las siguientes bibliotecas de R:\n\n- [`tidyverse`](https://www.tidyverse.org/)\n- [`rvest`](https://rvest.tidyverse.org/)\n- [`lubridate`](https://lubridate.tidyverse.org/)\n- [`tidytext`](https://juliasilge.github.io/tidytext/index.html)\n- [`tm`](https://cran.r-project.org/web/packages/tm/tm.pdf)\n- [`wordcloud2`](https://cran.r-project.org/web/packages/wordcloud2/wordcloud2.pdf)\n- [`wordcloud`](https://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf)\n- [`reshape2`](https://cran.r-project.org/web/packages/reshape2/reshape2.pdf)\n\nSi aún no tiene instalada estas bibliotecas puede ejecutar el siguiente código para instalarlas:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstall.packages(c(\n  \"tidyverse\",\n  \"rvest\",\n  \"lubridate\",\n  \"tidytext\",\n  \"tm\",\n  \"wordcloud\",\n  \"wordcloud2\",\n  \"reshape2\"\n),\ndependencies = TRUE)\n```\n:::\n\n\n# Script completo\n\nPara ver el código completo de este documento puede dar clic donde señala la flecha roja de la siguiente imagen:\n\n![](img1.PNG)\n\n# Función `googleNoticiasR()`\n\n- La función aquí presentada fue previamente discutida en la [sesión 02 del semillero.](https://www.youtube.com/watch?v=XY4zhqAfaSs&ab_channel=RPyCol) Si desea ver las diapositivas de esta sesión pueden ser consultadas [aquí.](https://rpubs.com/Edimer/981979)\n- El código completo está en [Github](https://github.com/web-edimer/semillero-ciencia-datos/blob/master/code-example/ejemplo-scraping-R-completo.R)\n- La función recibe como entrada (argumento) la `url` de [Google Noticias](https://news.google.com/home?hl=es-419&gl=CO&ceid=CO:es-419) desde la cual el usuario desea obtener las noticias.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngoogleNoticiasR <- function(url) {\n  titulo_noticia <-\n    url %>%\n    read_html() %>%\n    html_elements(\"body\") %>%\n    html_elements(xpath = '//a[@class = \"WwrzSb\"]')  %>%\n    html_attr(\"aria-label\")\n  \n  fuente_noticia <-\n    url %>%\n    read_html() %>%\n    html_elements(\"body\") %>%\n    html_elements(xpath = '//span[@class = \"vr1PYe\"]') %>%\n    html_text()\n  \n  fecha_noticia <-\n    url %>%\n    read_html() %>%\n    html_elements(\"body\") %>%\n    html_elements(xpath = '//time[@class = \"hvbAAd\"]') %>%\n    html_attr(\"datetime\") %>%\n    ymd_hms()\n  \n  df_noticias <-\n    data.frame(\n      noticia = titulo_noticia,\n      fuente = fuente_noticia,\n      fecha = fecha_noticia,\n      fecha_consulta = Sys.time()\n    )\n  \n  return(df_noticias)\n  \n}\n```\n:::\n\n\n# Bibliotecas de R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)  # manipulación de datos\nlibrary(rvest)      # web scraping\nlibrary(lubridate)  # manipulación de fechas\nlibrary(tidytext)   # procesamiento de texto\nlibrary(tm)         # stopWords \nlibrary(wordcloud)  # Nube de palabras\nlibrary(wordcloud2) # Nube de palabras\nlibrary(reshape2)   # Remodelamiento de datos\n```\n:::\n\n\n# Noticias\n\n- El usuario es libre de elegir el tipo de noticias que desea. En este caso vamos a utilizar las siguientes noticias:\n  - [Colombia](https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNREZzY3pJU0JtVnpMVFF4T1NnQVAB?hl=es-419&gl=CO&ceid=CO%3Aes-419)\n  - [Negocios](https://news.google.com/topics/CAAqLAgKIiZDQkFTRmdvSUwyMHZNRGx6TVdZU0JtVnpMVFF4T1JvQ1EwOG9BQVAB?hl=es-419&gl=CO&ceid=CO%3Aes-419)\n  - [Deportes](https://news.google.com/topics/CAAqLAgKIiZDQkFTRmdvSUwyMHZNRFp1ZEdvU0JtVnpMVFF4T1JvQ1EwOG9BQVAB?hl=es-419&gl=CO&ceid=CO%3Aes-419)\n- **Nota:** es posible ingresar cualquier otro tópico de interés, por ejemplo, [salud.](https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNR3QwTlRFU0JtVnpMVFF4T1NnQVAB?hl=es-419&gl=CO&ceid=CO%3Aes-419)\n\n## Colombia\n\nPrimero guardamos la URL para las noticas de Colombia en un objeto de nombre `url_colombia`. Cabe mencionar que este nombre lo asigna el usuario.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nurl_colombia <- \"https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNREZzY3pJU0JtVnpMVFF4T1NnQVAB?hl=es-419&gl=CO&ceid=CO%3Aes-419\"\n```\n:::\n\n\nLuego usamos la función `googleNoticiasR()` e ingresamos `url_colombia` como argumento de entrada. Guardamos este resultado en un objeto de nombre `noticias_colombia`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_colombia <- googleNoticiasR(url = url_colombia)\n```\n:::\n\n\nLa ejecución anterior devuelve un `dataframe` como se muestra a continuación. La función `head()` se utiliza para imprimir sólo las primeras 6 filas de la tabla.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_colombia %>% \n  head()\n```\n:::\n\n\nPodemos consultar el total de noticias (número de filas):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_colombia %>% \n  nrow()\n```\n:::\n\n\nLos nombres de la base de datos pueden ser consultados con la función `names()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_colombia %>% \n  names()\n```\n:::\n\n\n\n## Negocios\n\nPrimero guardamos la URL para las noticas de Colombia en un objeto de nombre `url_negocios`. Cabe mencionar que este nombre lo asigna el usuario.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nurl_negocios <- \"https://news.google.com/topics/CAAqLAgKIiZDQkFTRmdvSUwyMHZNRGx6TVdZU0JtVnpMVFF4T1JvQ1EwOG9BQVAB?hl=es-419&gl=CO&ceid=CO%3Aes-419\"\n```\n:::\n\n\nLuego usamos la función `googleNoticiasR()` e ingresamos `url_negocios` como argumento de entrada. Guardamos este resultado en un objeto de nombre `noticias_negocios`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_negocios <- googleNoticiasR(url = url_negocios)\nnoticias_negocios %>% \n  head()\n```\n:::\n\n\n## Deportes\n\nPrimero guardamos la URL para las noticas de Colombia en un objeto de nombre `url_deportes`. Cabe mencionar que este nombre lo asigna el usuario.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nurl_deportes <- \"https://news.google.com/topics/CAAqLAgKIiZDQkFTRmdvSUwyMHZNRFp1ZEdvU0JtVnpMVFF4T1JvQ1EwOG9BQVAB?hl=es-419&gl=CO&ceid=CO%3Aes-419\"\n```\n:::\n\n\nLuego usamos la función `googleNoticiasR()` e ingresamos `url_deportes` como argumento de entrada. Guardamos este resultado en un objeto de nombre `noticias_deportes`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_deportes <- googleNoticiasR(url = url_deportes)\nnoticias_deportes %>% \n  head()\n```\n:::\n\n\n# Análisis exploratorio\n\n- **Todo el análisis exploratorio será ilustrado con noticias de Colombia**, el usuario podrá replicar el ejemplo con los otros tópicos de interés.\n- Generalmente el [análisis exploratorio de datos](https://en.wikipedia.org/wiki/Exploratory_data_analysis) tiene como propósito revelar patrones de comportamiento, validar hipótesis, generar nuevas preguntas de investigación, detectar atipicidades, entre otras.\n- Nuestro análisis exploratorio estará orientado a responder a las siguientes preguntas:\n  - ¿Cuántas noticias hay para cada medio de comunicación?\n  - ¿Cuáles son las palabras más frecuentes en las noticias?\n  - ¿Podemos asignar algún sentimiento a las noticias en función de las palabras que contienen? **Nota importante:** múltiples léxicos de sentimientos están disponibles en internet, no obstante, para el lenguaje **español** es un poco reducida la disponibilidad. Por este motivo y en aras de la sencillez, utilizaremos el léxico [AFINN](http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html) con su [versión en español](https://raw.githubusercontent.com/jboscomendoza/rpubs/master/sentimientos_afinn/lexico_afinn.en.es.csv). El léxico AFINN asigna puntuaciones a las palabras, oscilando entre -5 y 5, donde las puntuaciones negativas indican un sentimiento negativo y las puntuaciones positivas indican un sentimiento positivo.\n\n\n## Noticias Colombia\n\n### Tokenización\n\n- El primer paso es convertir las noticias (*cadenas de texto*) en [tokens](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization), es decir, palabras individuales que aportan información a nuestro análisis. Este proceso se logra a través de la función `unnest_tokens()` de la biblioteca `tidytext`. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntokens_colombia <-\n  noticias_colombia %>% \n  unnest_tokens(output = \"token\", input = noticia)\n\ntokens_colombia %>% \n  head()\n```\n:::\n\n\nAlgunas palabras en la columna `token` no tienen propiedades informativas, por ejemplo, conectores, artículos, pronombres, preposiciones, etc. Es común en la minería de texto utilizar [*stop words*](https://es.wikipedia.org/wiki/Palabra_vac%C3%ADa) para cada lenguaje, en este caso para el castellano. Podemos acceder a estas palabras a través de la función `stopwords()` de la biblioteca `tm`. Es importante mencionar que es posible que queden algunas palabras que no son informativas, de tal manera que se recomienda profundizar más en este tema. \n\nAsignamos las *stop words* a un objeto de nombre `stop_spanish`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstop_spanish <- stopwords(kind = \"spanish\")\n```\n:::\n\n\nTenemos en total el siguiente número de *stop words* en español:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstop_spanish %>% \n  length()\n```\n:::\n\n\nAhoa filtramos las palabras de la columna `token` que están dentro de las palabras sin significado (*stop words*) y asignamos el resultado a un objeto de nombre `tokens_colombia_final`. Note que en la columna `token` quedan números, que eventualmente podrían ser filtrados para el análisis, no obstante, se recomienda profundizar en cuál debería ser la limpieza del texto más adecuada para su análisis. En este caso hacemos caso omiso de estos datos.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntokens_colombia_final <-\n  tokens_colombia %>%\n  filter(!token %in% stop_spanish) \n\ntokens_colombia_final %>% \n  head()\n```\n:::\n\n\n\n### Preguntas\n\n#### Pregunta 1\n\n- ¿Cuántas noticias hay para cada medio de comunicación? **Nota:** para respondera a esta pregunta no es estrictamente necesario tokenizar el texto, por tal motivo obtendremos el conteo a través del dataframe de nombre `noticias_colombia`. Observe que algunas fuentes se repiten, por ejemplo, `El Tiempo` y `EL TIEMPO`, `R` los define como entidades diferentes porque no están escritas de la misma manera, aunque esta característica es fácil de resolver lo dejaremos así y cada usuario podrá direccionar la depuración bajo la estructura correcta.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_colombia %>% \n  count(fuente, sort = TRUE)\n```\n:::\n\n\nPodemos graficar los 10 primeros medios de comunicación con mayor número de noticias:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_colombia %>% \n  count(fuente, sort = TRUE) %>% \n  slice(1:10) %>% \n  ggplot(aes(x = reorder(fuente, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = \"\", y = \"Noticias (n)\", title = \"Google Noticias - Colombia\")\n```\n:::\n\n\n#### Pregunta 2\n\n- ¿Cuáles son las palabras más frecuentes en las noticias? Para responder a estar preguna utilizaremos el dataframe que posee los tokens y sobre el cual ya pasamos el filtro de las palabras sin significado, es decir, `tokens_colombia_final`. Observamos que la palabra más frecuente en las noticias es \"petro\". \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntokens_colombia_final %>% \n  count(token, sort = TRUE)\n```\n:::\n\n\nComo son tantas palabras, es posible representar esta información a través de [nubes de palabras.](https://es.wikipedia.org/wiki/Nube_de_palabras) Este proceso lo llevamos a cabo con la biblioteca [`wordcloud2`.](https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.html)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntokens_colombia_final %>%\n  count(token, sort = TRUE) %>% \n  wordcloud2(data = ., backgroundColor = \"black\")\n```\n:::\n\n\n#### Pregunta 3\n\n- ¿Podemos asignar algún sentimiento a las noticias en función de las palabras que contienen? Para responder a esta pregunta lo primero que vamos a hacer es descargar el archivo que contiene el sentimiento para las palabras en español. Se puede descargar [aquí.](https://raw.githubusercontent.com/jboscomendoza/rpubs/master/sentimientos_afinn/lexico_afinn.en.es.csv)\n- Agradecimiento especial a [Juan Bosco Mendoza Vega](https://rpubs.com/jboscomendoza/) por disponibilizar esta información.\n- Note que la base de datos tiene tres columnas, la variable `Palabra` denota la información en español, la variable `Word` es su traducción al inglés y la `Puntuacion` (sin tilde) denota el *score* determinado por el léxico AFINN.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# URL\nurl_sentimiento <- \n  \"https://raw.githubusercontent.com/jboscomendoza/rpubs/master/sentimientos_afinn/lexico_afinn.en.es.csv\"\n\n# Lectura de datos\ndf_sentimiento <-\n  read_csv(url_sentimiento)\n\ndf_sentimiento %>% \n  head()\n```\n:::\n\n\nSi usted desea descargar el archivo anterior puede ejecutar el siguiente código:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndownload.file(url = url_sentimiento, destfile = \"datos_sentimiento_spanish_AFINN.csv\")\n```\n:::\n\n\nVamos a cambiar el nombre `Palabra` por `token`, para que podamos unir a la tabla `tokens_colombia_final` y seleccionamos sólo las variables `token` y `Puntuacion`. Además, discretizamos la variable `Puntuacion` en una nueva variable llamada `sentimiento`, de tal manera que si la `Puntuacion` es mayor a 0 se le asigna el nivel `Positivo`, de lo contrario será `Negativo`. Asignamos este resultado a un nuevo objeto de nombre `sentimiento_spanish`. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsentimiento_spanish <-\n  df_sentimiento %>% \n  rename(token = Palabra) %>% \n  select(token, Puntuacion) %>% \n  mutate(sentimiento = if_else(Puntuacion > 0, \"Positivo\", \"Negativo\"))\n\nsentimiento_spanish %>% \n  head()\n```\n:::\n\n\nAhora unimos los datos de sentimiento con la tabla `tokens_colombia_final`. La unión la realizamos con la función `inner_join()`, de tal manera que sólo serán tenidas en cuenta palabras que estén en ambas tablas. Note que la nueva tabla se reduce, ya que muchas palabras de las noticias no están presente en el dataframe `sentimiento_spanish`. Es importante mencionar que esta es una aproximación simple de análisis de sentimientos, sin embargo, podrían ser utilizadas técnicas más robustas, por ejemplo, [Deep Learning.](https://towardsdatascience.com/an-easy-tutorial-about-sentiment-analysis-with-deep-learning-and-keras-2bf52b9cba91)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_sentimiento <-\n  inner_join(x = tokens_colombia_final, y = sentimiento_spanish, by = \"token\")\n\nnoticias_sentimiento %>% \n  head()\n```\n:::\n\n\nPodemos consultar el número de filas de la nueva tabla.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_sentimiento %>% \n  nrow()\n```\n:::\n\n\nPodemos responder a la siguiente pregunta, ¿Predominan las palabras positivas o negativas? Parece que son más las noticias que tiene palabras negativas que positivas.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_sentimiento %>% \n  count(sentimiento)\n```\n:::\n\n\n¿Cuál medio de comunicación es más negativo o positivo en sus noticias?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_sentimiento %>% \n  count(fuente, sentimiento, sort = TRUE)\n```\n:::\n\n\nPodemos representar el resultado anterior a través de un gráfico. Para tener una representación más transparente filtramos medios de comunicación con más de 3 noticias.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_sentimiento %>% \n  count(fuente, sentimiento, sort = TRUE) %>% \n  filter(n > 3) %>% \n  ggplot(aes(x = reorder(fuente, n), y = n, fill = sentimiento)) +\n  geom_col(position = \"fill\") +\n  coord_flip() +\n  labs(x = \"\", y = \"Frecuencia relativa\", fill = \"Sentimiento\") +\n  theme(legend.position = \"top\")\n```\n:::\n\n\nHasta ahora hemos usamos la variable `sentimiento`, pero también podríamos calcular alguna métrica estadística con la variable `Puntuacion`. En este caso calculamos la mediana de la `Puntuacion` y obtenemos el número de datos (`N`) con los cuales es calculada la métrica.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_sentimiento %>% \n  group_by(fuente) %>% \n  summarise(\n    mediana_sent = median(Puntuacion),\n    N = n()\n  ) %>% \n  arrange(desc(mediana_sent))\n```\n:::\n\n\nPodemos graficar la nube de palabras para el sentimiento positivo y negativo a través de la biblioteca `wordcloud`. Para este proceso fíjese que \"remodelamos\" los datos a través de la función `acast()` de la biblioteca `reshape2`. Este proceso es necesario para obtener la nube de palabras comparativa con la función `comparison.cloud()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnoticias_sentimiento %>%\n  count(token, sentimiento, sort = TRUE) %>% \n  acast(token ~ sentimiento, value.var = \"n\", fill = 0) %>%\n  comparison.cloud(colors = c(\"gray20\", \"forestgreen\"),\n                   max.words = 100)\n```\n:::\n\n\n\n# Análisis de sentimientos\n\nPara tener contexto de lo que signfica el *análisis de sentimientos*, se recomienda revisar los siguientes recursos de información:\n\n- [Sentiment analysis (wikipedia)](https://en.wikipedia.org/wiki/Sentiment_analysis)\n- [Sentiment analysis with tidy data](https://www.tidytextmining.com/sentiment.html)\n- [Python Sentiment Analysis Tutorial](https://www.datacamp.com/tutorial/simplifying-sentiment-analysis-python)\n- [Análisis de texto (text mining) con Python](https://www.cienciadedatos.net/documentos/py25-text-mining-python.html)\n- [TEDx: Análisis de sentimientos](https://www.youtube.com/watch?v=n4L5hHFcGVk&ab_channel=TEDxTalks)",
    "supporting": [
      "GoogleNoticias-R_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}